{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Framing the Problem \n",
    "In this tutorial, we will learn to build simple feed foward neural networks using Keras. Keras is a popular high-level deep learning API that can use either Theano or Tensorflow as a backend. Keras objects tend to represent layers and whole neural networks, as oppossed to the single operations found in either backend. You can install keras with the command pip install keras. For more details about the API, please refer to https://keras.io/\n",
    "\n",
    "As usual, we first import some python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function               # Allows for python3 printing\n",
    "import keras \n",
    "from keras.models import Sequential                 # import the sequential model, which is a core object in Keras\n",
    "from keras.layers import Dense, Activation          # import the dense layer, AKA the fully connected layer\n",
    "                                                    # and Keras' library of activation functions\n",
    "from keras.optimizers import RMSprop                    #  import RMSprop as optimizer\n",
    "\n",
    "import pandas                                       # Data storage\n",
    "from sklearn.model_selection import train_test_split# Splits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has two classes of abstract models - the “Sequential” model is an object to which you can append layers. and is simpler. The other is the “Model” class used with functional API, which is capable of building more complex models. Here we start from sequential models. In project 2, you will learn how to use functional API. \n",
    "\n",
    "## 2. Get the data\n",
    "\n",
    "You should download the dataset from the github repository and make sure both \"train.csv\" and \"test.csv\" is in the same folder with this jupyter notebook file. The original dataset is from the Kaggle Competition: digital recognizer (https://www.kaggle.com/c/digit-recognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv(\"train.csv\") # Read in data\n",
    "dataset = dataset.as_matrix() # Convert to ndarray\n",
    "X,y = dataset[:,1:], dataset[:,0] # Separate data points and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train.csv's format:\n",
    "\n",
    "Label,pixel0,pixel1,...,pixel783\n",
    "\n",
    "1,0,0,...,0\n",
    "\n",
    "`read_csv:` Will read a csv file and create a table for it.\n",
    "\n",
    "`as_matrix:` Will convert the table into a ndarray for numpy operations\n",
    "\n",
    "We read in our data and convert to ndarray for fast numpy operations. We also separate the data into two parts: feature vectors and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the data\n",
    "\n",
    "In this section we use code to gather more information about our data. Usually the most important things we want to know about the data are: \n",
    "1. the dimension/shape of the data\n",
    "2. range of the data\n",
    "3. visualization of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features is: (42000, 784)\n",
      "the shape of labels is: (42000,)\n",
      "the range of features is: 0 to 255\n",
      "the range of labels is: 0 to 9\n",
      "label 1 is 1\n",
      "label 2 is 0\n",
      "label 3 is 1\n",
      "label 4 is 4\n",
      "label 5 is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print ('the shape of features is:',X.shape)       #print data shape\n",
    "print ('the shape of labels is:',y.shape)       #print data shape\n",
    "\n",
    "import numpy as np\n",
    "print ('the range of features is:',np.min(X),'to',np.max(X))\n",
    "print ('the range of labels is:',np.min(y),'to',np.max(y))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "for i in range(5):\n",
    "    image = X[i]\n",
    "    plt.subplot(1,10, i+1)\n",
    "    image = image.reshape(28,28)\n",
    "    print ('label', i+1, 'is',y[i])\n",
    "    plt.imshow(image, cmap='Greys')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing data are 28 by 28 images, and thus the total number of features is 28 x 28 = 784.\n",
    "\n",
    "## 4. Prepare the data\n",
    "Here we need to divide it by 255 for normalization, since the features are pixels value from 0 to 255. After that, we will use function from keras **utils.to.categoriacal** to do one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.20) # Split data\n",
    "\n",
    "x_train = x_train/255.0                           # normalize training data\n",
    "x_val = x_val/255.0                             # normalize testing data\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create the model\n",
    "We want to set several parameters at the beginning so that we can easily see them and change them. It would also be clear to others when reading your code and try to see what parameter you used. Here the most important parameters to set are training epochs, batch size, and learning rate. \n",
    "\n",
    "Batch size refers to the portion of the entire sample that you want to put into the neural network for training. One epoch is when all samples are trained once. Learning rate is how fast you want the model to train, but large learning rates would likely to cause problems such as missing the optimal solution (overshoot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 868,106\n",
      "Trainable params: 868,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "batch_size = 256\n",
    "epochs = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# build the model\n",
    "model = Sequential()                                         # define model to be sequential\n",
    "model.add(Dense(256, activation='relu',input_dim=784))       # first hidden layer with 256 neurons\n",
    "model.add(Dense(512, activation='relu'))                     # second hidden layer with 512 neurons\n",
    "model.add(Dense(1024, activation='relu'))                     # third hidden layer with 1024 neurons\n",
    "model.add(Dense(10, activation='softmax'))                   # output layer\n",
    "model.summary()                                              # print out summary for all layers \n",
    "\n",
    "\n",
    "my_optimizer = keras.optimizers.RMSprop(lr=learning_rate)                   # using learning rate 0.001\n",
    "model.compile(optimizer=my_optimizer,                        # using SGD with our set lr as optimizer\n",
    "              loss='categorical_crossentropy',               # using cross entropy loss\n",
    "              metrics=['accuracy'])                          # metric that is called during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set batch size to 256 and use 15 epochs. A large batch size might result to \"GPU out of memory\" issue, depending on the data size. We can set to a smaller batch size if the dataset is large. For epochs, the larger the number of epochs is, the more time we will need to train the model. We also used learning rate 0.001. Usually, the learning rate ranges from 0.01 to $10^{-7}$\n",
    "\n",
    "When building the model, the first line should be **model = Sequential() **. This line in itializes a special model object called the Sequential model. This object represents our overall neural network, and it contains a list of every layer in our network, in order. We can add and remove layers to our neural network by using the model.add() method, which expects a layer object as an input, and appends that layer to the model.\n",
    "We need to be careful when using a sequential model, as the order in which we add layers defines our network. We need to make sure that the 'add()' statements are used in the order that represents the network we want.\n",
    "\n",
    "In this example, we are adding two Dense layers, each with 256 neurons and activated with the rectified linear function, follows by a 10-class softmax layer.\n",
    "Whenever we use the sequential model class, we need to define the expected shape of the input vectors to our model, and we do this with the keyword input_dim, passed into the first layer of our model. We can ONLY use input_dim in the _first_ layer of a model; trying to pass in this keyword to a later layer will throw an error.\n",
    "\n",
    "Within Keras, the name for the classic, fully-connected hidden layer is the Dense Layer. Here, we are adding two such layers. This network is fully in keeping with the material shown and discussed in the neural network chapter, and in fact we are defining exactly the kind of model you'd expect if that was the only source you had on the topic. \n",
    "\n",
    "Tensorflow and Theano both require that we compile the model or function we've defined. Keras, being built on those libraries, also has this requirement.\n",
    "\n",
    "Up to the line reading 'model.summary()', we are _defining_ the unique model that we want to use. To implement this into a runnable program we must compile it. This is done with the model.compile() method. Compile() expects three arguments at least, which are the training algorithm that we want to use, the cost or loss function that is to be optimized during training, and a metric that we want the model to use during evalution.\n",
    "\n",
    "Once a model is compiled, it will have several methods available. These include fit(), which expects a set of training points and their labels, and which trains using the loss and optimizer defined during compilation. A compiled model will also have a predict() method, which expects a set of datapoints, and returns predicted labels for the (hopefully never before seen) points. Finally, the compiled model will have the evaluate() method, expects a testing set and its labels, and which returns both the loss and 'metrics' defined at compilation, evaluated on a new set of given points and labels.\n",
    "\n",
    "Here, we used the stochastic gradient descent optimizer, with the standard categorical_crossentropy cost function. For other optimizers check out [Keras:optimizers](https://keras.io/optimizers/). We decided on accuracy as an evaluation metric as this is standard practice; we're concerned with how many test images we will ultimately get right. \n",
    "\n",
    "The output of model.summary is a table specifying layers in this model and number of parameters per layer. The number of parameters is calculated with equation (# inputs)x(# neurons)+(# neurons). For example, there are 784x256+256, or 200960 parameters in the first layer.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 train the model\n",
    "In the next cell, we train the compiled model using **model.fit**.\n",
    "The first two input arguments should be training data and label, where the number of columns in the training data must match the input_shape keyword used in the first layer, and the number of columns in the label variable must equal the number of ouputs in our final softmax layer. In this case, the number of columns in x_train must be 784, and the number of columns in y_train must be 10. \n",
    "\n",
    "The remaning arguments are keywords that we have defined variables for already. We use the batch size and epochs specified before. Verbose refers to the depth of the 'progress reports' that keras prints at every training iteration, where 0 means no printout, 1 means a progress bar that summarizes epochs, and 2 means print out a line for every epoch. We have already defined a validation set, which we've called x_test, in a previous cell. \n",
    "\n",
    "As we've learned, it's important to regularize neural networks. There are many methods available for this, but here we will implement the method that is especially important for neural networks, namely _early stopping._ As it turns out, Keras has a built in method for early stopping. This is stored within the callbacks module. Callbacks are a series of functions that are run after every training iteration, that decide how trainin should proceed based on what has already occurred. Early stopping just causes the training process to end early if improvement hasn't been seen in a set number of epochs.\n",
    "Besides early stopping, we need a callback that keeps track of the best performing set of weights. We use Keras' ModelCheckpoint callback to this end. This callback by default stores the current weights and their performance in a file after every iteration. We override the default, causing the method to ONLY store the best performing weights in the whole history of training, so that when training stops we can return the weights to their best performing values.\n",
    "\n",
    "For both callbacks, we are defining performance as validation _accuracy_. The validation set is defined in the fit method, and the callbacks \n",
    "are using the keras default keyword 'val_acc' to refer to accuracy on this set.\n",
    "\n",
    "We define our callbacks and set the parameters for each separately. We then pass the callbacks into the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/32\n",
      " - 7s - loss: 0.8102 - acc: 0.8074 - val_loss: 0.4368 - val_acc: 0.8542\n",
      "Epoch 2/32\n",
      " - 7s - loss: 0.3069 - acc: 0.9109 - val_loss: 0.2726 - val_acc: 0.9182\n",
      "Epoch 3/32\n",
      " - 7s - loss: 0.2412 - acc: 0.9291 - val_loss: 0.2253 - val_acc: 0.9327\n",
      "Epoch 4/32\n",
      " - 7s - loss: 0.2020 - acc: 0.9408 - val_loss: 0.2128 - val_acc: 0.9381\n",
      "Epoch 5/32\n",
      " - 7s - loss: 0.1731 - acc: 0.9494 - val_loss: 0.1927 - val_acc: 0.9408\n",
      "Epoch 6/32\n",
      " - 7s - loss: 0.1517 - acc: 0.9557 - val_loss: 0.1565 - val_acc: 0.9545\n",
      "Epoch 7/32\n",
      " - 7s - loss: 0.1322 - acc: 0.9619 - val_loss: 0.1541 - val_acc: 0.9527\n",
      "Epoch 8/32\n",
      " - 7s - loss: 0.1185 - acc: 0.9665 - val_loss: 0.1436 - val_acc: 0.9555\n",
      "Epoch 9/32\n",
      " - 7s - loss: 0.1055 - acc: 0.9702 - val_loss: 0.1917 - val_acc: 0.9371\n",
      "Epoch 10/32\n",
      " - 7s - loss: 0.0942 - acc: 0.9730 - val_loss: 0.1273 - val_acc: 0.9621\n",
      "Epoch 11/32\n",
      " - 7s - loss: 0.0843 - acc: 0.9756 - val_loss: 0.1194 - val_acc: 0.9632\n",
      "Epoch 12/32\n",
      " - 7s - loss: 0.0771 - acc: 0.9779 - val_loss: 0.1085 - val_acc: 0.9663\n",
      "Epoch 13/32\n",
      " - 7s - loss: 0.0684 - acc: 0.9806 - val_loss: 0.1320 - val_acc: 0.9592\n",
      "Epoch 14/32\n",
      " - 7s - loss: 0.0626 - acc: 0.9821 - val_loss: 0.1077 - val_acc: 0.9679\n",
      "Epoch 15/32\n",
      " - 7s - loss: 0.0558 - acc: 0.9843 - val_loss: 0.1252 - val_acc: 0.9621\n",
      "Epoch 16/32\n",
      " - 7s - loss: 0.0510 - acc: 0.9856 - val_loss: 0.1042 - val_acc: 0.9679\n",
      "Epoch 17/32\n",
      " - 7s - loss: 0.0456 - acc: 0.9871 - val_loss: 0.1261 - val_acc: 0.9621\n",
      "Epoch 18/32\n",
      " - 7s - loss: 0.0401 - acc: 0.9890 - val_loss: 0.0988 - val_acc: 0.9704\n",
      "Epoch 19/32\n",
      " - 7s - loss: 0.0365 - acc: 0.9901 - val_loss: 0.0950 - val_acc: 0.9700\n",
      "Epoch 20/32\n",
      " - 7s - loss: 0.0328 - acc: 0.9913 - val_loss: 0.0966 - val_acc: 0.9692\n",
      "Epoch 21/32\n",
      " - 7s - loss: 0.0293 - acc: 0.9924 - val_loss: 0.0971 - val_acc: 0.9710\n",
      "Epoch 22/32\n",
      " - 7s - loss: 0.0263 - acc: 0.9938 - val_loss: 0.1230 - val_acc: 0.9619\n",
      "Epoch 23/32\n",
      " - 7s - loss: 0.0238 - acc: 0.9941 - val_loss: 0.1042 - val_acc: 0.9687\n",
      "Epoch 24/32\n",
      " - 7s - loss: 0.0213 - acc: 0.9949 - val_loss: 0.1066 - val_acc: 0.9694\n",
      "Epoch 25/32\n",
      " - 7s - loss: 0.0185 - acc: 0.9957 - val_loss: 0.1200 - val_acc: 0.9668\n",
      "Epoch 26/32\n",
      " - 7s - loss: 0.0164 - acc: 0.9963 - val_loss: 0.1033 - val_acc: 0.9701\n"
     ]
    }
   ],
   "source": [
    "best_weights_filepath = './best_weights.hdf5' ##define the filename to store\n",
    "                                            ##the best performance and weights\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                              patience = 5) \n",
    "#Stop training early if val_acc doesn't improve for 5 epochs\n",
    "\n",
    "SaveBestWeights = keras.callbacks.ModelCheckpoint(best_weights_filepath,\n",
    "                                                  monitor='val_acc',\n",
    "                                                  save_best_only=True)\n",
    "# store the historically best performing weights in best_weights_filepath\n",
    "#, where performance is given by accuracy on the validation set.\n",
    "\n",
    "\n",
    "model_history = model.fit(x_train, y_train,                   # training data \n",
    "                    batch_size=batch_size,                   # batch size 256\n",
    "                    epochs=epochs,                           # 300 epochs \n",
    "                    verbose= 2,                              # verbose level\n",
    "                    validation_data = (x_val, y_val),  #Use the previously defined x_test as a validation set. \n",
    "                    callbacks = [earlyStopping, SaveBestWeights]\n",
    "                         )     \n",
    "model.load_weights(best_weights_filepath) ##Set the best performing weights to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the model:Submission to Kaggle\n",
    "We first need to read in the data and then convert it into a numpy array so we can perform numpy operations on it. Next, we will make predictions for the Kaggle test set. We cannot evaluate our performance on this test set, as Kaggle keeps these a secret so that we can't cheat. (Cheating is actually really easy, but they're really good at telling when it happens)\n",
    "\n",
    "To be clear, we defined an evaluation set above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pandas.read_csv(\"test.csv\")             # Read data\n",
    "testset = testset.as_matrix()                     # Convert to ndarray\n",
    "testset = testset/255.0                             # normalize testing data\n",
    "predictions = model.predict_classes(testset)           # Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pandas.DataFrame(data=predictions, index=np.arange(1,len(predictions)+1), columns=['Label']) # Create dataframe\n",
    "submission.index.name = 'ImageId' # Set index name\n",
    "\n",
    "csv_text = submission.to_csv() # Convert to text\n",
    "\n",
    "# Write to file 'submission.csv'\n",
    "with open(\"submission1.csv\",'w') as csv_file:\n",
    "    csv_file.write(csv_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle requires our submission to be in the form:\n",
    "\n",
    "ImageId,Label\n",
    "\n",
    "1,1\n",
    "\n",
    "2,8\n",
    "\n",
    ".... \n",
    "\n",
    "\n",
    "`DataFrame:` Pandas table to store data\n",
    "    \n",
    "    data: data to be stored in table\n",
    "     \n",
    "    index: indexes for table\n",
    "    \n",
    "    columns: labels for table columns\n",
    "\n",
    "We create a DataFrame to store our results and set the index name to be ImageId. We also set the indexes for the result to start at 1 and end at the length of the result.\n",
    "\n",
    "`to_csv:` Converts DataFrame into text\n",
    "\n",
    "We convert the DataFrame into text and write the text into csv file. After submitting our predictions to Kaggle, we are informed by them of our accuracy on the testing set. \n",
    "Again, you can submitt your own predictions to https://www.kaggle.com/c/digit-recognizer\n",
    "\n",
    "## Conclusion\n",
    "In this tutorial we have learned how to define, compile, train, evaluate, and make predictions with Keras neural networks. There are many more methods that we could use in _all_ of these stages.\n",
    "\n",
    "Keras offers many layer types, different compile options, many optimizers and loss functions, and many more callback methods that those used here.  With the Keras API, one could build, or implement, just about any theoretically possible neural network. Every cutting edge method has some simple implementation in the API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
