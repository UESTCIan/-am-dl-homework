{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frame the problem \n",
    "In this tutorial, we will learn to build a simple recurrent neural network using Keras. We assume that you have already installed Keras when studying the last CNN project. For more details about the API, please refer to https://keras.io/\n",
    "First, we will import some python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python3.5.2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function                  # Allows for python3 printing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, LSTM, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has two kinds of models, the Sequential model is a linear stack of layers and is simpler. The other is the Model class used with functional API, which is capable to build more complex models. In project 1, we have learned how to use the Sequential model, and in project 2 we used functional API. Here we will go back to the Sequential model for demonstration purpose.  \n",
    "\n",
    "## 2. Get the data\n",
    "Here we use the text file in this github repo. The raw text was downloaded from https://archive.org/details/shakespearessonn01041gut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sonnet.txt\"\n",
    "#read the file and make all chracaters lowercase\n",
    "text = open(filename).read().lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert all characters to lowercase, so as to avoid distinguishing between lower and upper case letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the data\n",
    "\n",
    "In this section, we want to gather more information about our data. We are interested in, among other things, the total length of the text (how many characters are in the file) and the number of unique characters used (which will be 26 letters and a few symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 95690\n",
      "total chars: 38\n",
      "['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "print('text length:', len(text))    # this prints the length of the text \n",
    "\n",
    "chars = sorted(list(set(text)))    # sorted():Return a new list containing all items from the iterable in ascending order.\n",
    "print('total chars:', len(chars))\n",
    "print(chars)\n",
    "##This should print 38."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the data\n",
    "First, we need to encode all of the characters in our corpus as numbers. Here we use the python dictionary class, or dict, to allow for efficient interchange between numbers and characters.\n",
    "\n",
    "Recall that a dictionary is a set of key and value pairs, where keys can be used to efficiently get an associated value. Dictionaries are much faster and more flexible than lists, which are limited by index logic.\n",
    "\n",
    "my_dict = {'name': 'John', 1: [2, 4, 3]}\n",
    "\n",
    "In the following code we construct two dictionaries. One allows us to efficiently map from characters in our corpus to an index, and the other allows us to go from an index to the character at that index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 encoding and decoding dictionaries\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 encoding and decoding dictionary\n",
    "\n",
    "\"char_indices\" is a dictionary that returns the index for any input character.\n",
    "'indices_char' does the oppostive, returning the character that corresponds to an input index. \n",
    "\n",
    "char_indices[‘a’] = (index of a), indices_char[n] = ‘(character indexed by n)’\n",
    "\n",
    "We'll convert chars to an RNN readable format with char_indices, and we'll convert our RNN's outputs to characters with indices_char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 cut corpus into equal length sequences\n",
    "maxlen = 40\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])               \n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)   #generate zeros with size[number of sentences,40,38]\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool) #generate zeros with size[number of sentences,38]\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 cut the corpus into equal length sequences####\n",
    "We also want to have the text ordered into equal length sequences. (There are multiple ways to do this, but for simplicity we just choose sequences of a set number of characters). Recall that an RNN calculates the gradient with respect to every copy of the network, for every step in every sequence. Thus the longer the length of the sequences, or the more sequences we have, the longer it takes to train. \n",
    "\n",
    "We choose 40 characters for each sequence and set the step size to 3 to decrease the total number of sequences by a factor of 3. If we chose a step size of 1, then our dataset would be composed of every possible 40 character subsequence of the corpus. \n",
    "\n",
    "We want the input to our RNN to be a sequence, and the output to be the character that immediately follows that sequence. We can think of this as 'completing' a sequence.\n",
    "\n",
    "The last step is to convert to one-hot encoding for both the input and the output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Create the model\n",
    "In this section, we will create a very basic recurrent neural network with one RNN layer. The input shape is the shape of any input sequence in our training set, which is the second and third dimensions of x: 40 by 38 (the maximum length of the sentence is 40, and after one-hot encoding, every character is 1 by 38). The number of neurons is user-defined.\n",
    "\n",
    "We also need a Fully-connected layer for classification (predicting the last character). We feed our first RNN into this fully connected layer. This means that our output layer is a 38-unit softmax layer which corresponds to the 38 available characters.\n",
    "\n",
    "Finally, we use **model.compile** to configure the model. We use the categorical cross entropy loss function, owing to the mutually exclusive nature of the classes we're predicting. We arbitrarily pick the Adam optimizer (you can use other optimizers to see if the result will be improved). \n",
    "For details of other optimizers check [Keras: optimizers](https://keras.io/optimizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Train the model and predict the output\n",
    "Here we call our model train it using **.fit**.\n",
    "\n",
    "We used a for-loop here so that we can generate text every few epochs of training (specifically after every ten.) \n",
    "\n",
    "To generate the sentence, we will randomly choose a sequence from the corpus and feed it into the model. The returned character is the model's prediction for the next character.\n",
    "RNNs can be run for arbitrarily many steps after an input sequence, and we do this to generate 400 characters after the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 23s 728us/step - loss: 2.9885\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 19s 595us/step - loss: 2.7253\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 20s 614us/step - loss: 2.4905\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 19s 605us/step - loss: 2.3635\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 18s 561us/step - loss: 2.2804\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 18s 553us/step - loss: 2.2150\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 19s 591us/step - loss: 2.16310s - loss: 2.\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 19s 586us/step - loss: 2.1186\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 19s 596us/step - loss: 2.0810\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 19s 586us/step - loss: 2.0481\n",
      "\n",
      "Iteration 1\n",
      " the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the seare,\n",
      "and the hath the hath the hath the\n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 19s 608us/step - loss: 2.0184\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 19s 608us/step - loss: 1.9919\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 19s 605us/step - loss: 1.9691\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 19s 608us/step - loss: 1.9465\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 19s 599us/step - loss: 1.9259\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 19s 603us/step - loss: 1.9055\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 19s 586us/step - loss: 1.8877\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 19s 608us/step - loss: 1.8704\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 19s 592us/step - loss: 1.8536\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 20s 618us/step - loss: 1.8378\n",
      "\n",
      "Iteration 2\n",
      "the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the \n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 19s 605us/step - loss: 1.8228\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 19s 601us/step - loss: 1.8087\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 20s 615us/step - loss: 1.7939\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 19s 609us/step - loss: 1.7803\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 19s 606us/step - loss: 1.7674\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 19s 595us/step - loss: 1.7558\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 19s 605us/step - loss: 1.7440\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 19s 600us/step - loss: 1.7308\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 19s 588us/step - loss: 1.7193\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 19s 604us/step - loss: 1.7086\n",
      "\n",
      "Iteration 3\n",
      "e the with thee,\n",
      "and the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with t\n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 19s 585us/step - loss: 1.6976\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 21s 662us/step - loss: 1.6876\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 20s 633us/step - loss: 1.6773\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 21s 645us/step - loss: 1.6673\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 22s 676us/step - loss: 1.6567\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 20s 620us/step - loss: 1.64631s -\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 19s 588us/step - loss: 1.6374\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 22s 694us/step - loss: 1.6281\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 22s 683us/step - loss: 1.6199\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 22s 696us/step - loss: 1.6104\n",
      "\n",
      "Iteration 4\n",
      "\n",
      "  live the self the war the sters the prease,\n",
      "and the self all the self all the self deart.\n",
      "\n",
      "xxx\n",
      "\n",
      "the with the will shall the wall shall me stall,\n",
      "the with the sterse and the stall my see,\n",
      "  and the self all the self all the self all me so sell\n",
      "to the self the war the sters the prous see,\n",
      "the dost that thou and the sters the proase\n",
      "that i and the self all the self all me so sell\n",
      "to the self the w\n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 22s 682us/step - loss: 1.6008\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 20s 642us/step - loss: 1.5928\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 20s 626us/step - loss: 1.5848\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 19s 583us/step - loss: 1.5750\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 18s 564us/step - loss: 1.5676\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 18s 556us/step - loss: 1.5579\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 18s 571us/step - loss: 1.5503\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 18s 565us/step - loss: 1.5414\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 18s 559us/step - loss: 1.53251s\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 18s 555us/step - loss: 1.5252\n",
      "\n",
      "Iteration 5\n",
      " deart,\n",
      "and thee shee the sters the still the sordery\n",
      "on the some that with the worth the prouse\n",
      "that i mand thee the sters the still me all thee thee,\n",
      "  and the seet and then i love the stersery still,\n",
      "that i love to the will for thy shall,\n",
      "the some the sters the still the some thee,\n",
      "  and the seet and then i mase the sters,\n",
      "and the some the sters the stor'd the prowe,\n",
      "and the some the sters the \n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 18s 572us/step - loss: 1.5161\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 19s 596us/step - loss: 1.5084\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 18s 580us/step - loss: 1.5003\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 19s 589us/step - loss: 1.4921\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 19s 585us/step - loss: 1.4845\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 19s 593us/step - loss: 1.4755\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 20s 623us/step - loss: 1.4689\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 19s 585us/step - loss: 1.4614\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 19s 580us/step - loss: 1.4520\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 19s 591us/step - loss: 1.44450s - l\n",
      "\n",
      "Iteration 6\n",
      "xxiii\n",
      "\n",
      "that with the worth that thou art my bear,\n",
      "  then the worth that thou art my bear to the,\n",
      "the earth the proud that thou art my love,\n",
      "  and the prouth's with the worth the part,\n",
      "when the sters the proud that which thou strong,\n",
      "and the proust for the self that thou art my deart,\n",
      "and the sterseres such steet of the wort,\n",
      "which the sterse of thy sure that thou art my me,\n",
      "  then the worth that t\n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 18s 578us/step - loss: 1.4367\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 19s 586us/step - loss: 1.4287\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 18s 562us/step - loss: 1.4217\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 18s 578us/step - loss: 1.4144\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31884/31884 [==============================] - 19s 590us/step - loss: 1.4070\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 19s 583us/step - loss: 1.3988\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 18s 580us/step - loss: 1.3920\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 19s 596us/step - loss: 1.3848\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - ETA: 0s - loss: 1.376 - 19s 596us/step - loss: 1.3769\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 19s 588us/step - loss: 1.3699\n",
      "\n",
      "Iteration 7\n",
      "row the world deay,\n",
      "  see the sterseres sube all the will shee strengess peart,\n",
      "and their thee the pood do have the sor fars,\n",
      "and all their their thee shell shall thee greaser's with,\n",
      "and thoughts that thou as me thou art for whill sporters says\n",
      "the love the pood that i me beauty shill,\n",
      "o that i whit still that thou shel strent\n",
      "or prids speaking the less to the selfal aupless seat\n",
      "so beauty's sume\n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 19s 598us/step - loss: 1.3629\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 19s 604us/step - loss: 1.3560\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 19s 600us/step - loss: 1.3486\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 19s 588us/step - loss: 1.3415\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 19s 606us/step - loss: 1.3342\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 19s 596us/step - loss: 1.3272\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 19s 607us/step - loss: 1.3196\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 20s 614us/step - loss: 1.3134\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 21s 643us/step - loss: 1.3065\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 19s 595us/step - loss: 1.2989\n",
      "\n",
      "Iteration 8\n",
      "ve's be,\n",
      "that i love to the self that thought\n",
      "in the world desplenieng ele,\n",
      "that enould not my be thee shee some in thee,\n",
      "  and though i mand thee thou astand glows,\n",
      "which i same grow the would do prosse.\n",
      "\n",
      "cxxii\n",
      "\n",
      "if that thou shal the with on that forth doth prease,\n",
      "and thee their sid can on the eaven that sure steet\n",
      "or prom my heart doth some in the remore,\n",
      "and thou astand with that thou art my b\n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 20s 613us/step - loss: 1.2906\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 20s 633us/step - loss: 1.2860\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 21s 659us/step - loss: 1.2784\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 23s 720us/step - loss: 1.2728\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 19s 598us/step - loss: 1.2657\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 19s 602us/step - loss: 1.2591\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 20s 615us/step - loss: 1.2533\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 18s 567us/step - loss: 1.2474\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 21s 650us/step - loss: 1.2406\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 18s 570us/step - loss: 1.2350\n",
      "\n",
      "Iteration 9\n",
      "s the pood that i mand \n",
      "for to now when ser his dud,\n",
      "that for the see the worth shall their sheet\n",
      "do dost the self-to thoug that i love thee,\n",
      "  and the seeting of thee that i whin sher did\n",
      "that thou art my bear the words of the remours,\n",
      "and be the still wall my fort for love.\n",
      "and his spornter shall shat thou shel shee to the,\n",
      "  thy bear my best not for my heart doth be,\n",
      "  as so stent of the with o\n",
      "\n",
      "Epoch 1/10\n",
      "31884/31884 [==============================] - 18s 566us/step - loss: 1.2268\n",
      "Epoch 2/10\n",
      "31884/31884 [==============================] - 18s 574us/step - loss: 1.2227\n",
      "Epoch 3/10\n",
      "31884/31884 [==============================] - 20s 626us/step - loss: 1.2151\n",
      "Epoch 4/10\n",
      "31884/31884 [==============================] - 22s 691us/step - loss: 1.2095\n",
      "Epoch 5/10\n",
      "31884/31884 [==============================] - 20s 630us/step - loss: 1.2030\n",
      "Epoch 6/10\n",
      "31884/31884 [==============================] - 21s 661us/step - loss: 1.1976\n",
      "Epoch 7/10\n",
      "31884/31884 [==============================] - 20s 630us/step - loss: 1.1926\n",
      "Epoch 8/10\n",
      "31884/31884 [==============================] - 20s 621us/step - loss: 1.1873\n",
      "Epoch 9/10\n",
      "31884/31884 [==============================] - 20s 620us/step - loss: 1.1801\n",
      "Epoch 10/10\n",
      "31884/31884 [==============================] - 18s 563us/step - loss: 1.1748\n",
      "\n",
      "Iteration 10\n",
      " of true,\n",
      "flaith tounsing will but the proawst of, ald thou arins of true\n",
      "that if theire, and thoull, doth speatter with dight,\n",
      "and though it his summer sweetest kind of tour,\n",
      "and that i whink with the worth shall still,\n",
      "o tam for his, i whill, and thoull,\n",
      "the beauty's suigh thee the party of love,\n",
      "and thine, sweet tounter wath thought of the wort,\n",
      "that i my love, not the worth spending tourshess \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 11):\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=10)\n",
    "    \n",
    "# generating text\n",
    "    print('\\nIteration', iteration)\n",
    "    start_index = np.random.randint(0, len(text) - maxlen - 1)\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]      #predict using model, generating a matrix of relu\n",
    "        next_index = np.argmax(preds)                    #we want the index with highest probability \n",
    "        next_char = indices_char[next_index]             #convert number back to character using the dictionary\n",
    "        sentence = sentence[1:] + next_char              #append the character predicted to the sentence (start from the second character)\n",
    "\n",
    "        sys.stdout.write(next_char)                \n",
    "        sys.stdout.flush()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss decreases over the training, and the text generated improves each time. The result is very impressive given a simple RNN model. Generally speaking, using an improved version of RNN model, such as LSTM, will make the result a lot better. We will leave the process of improving the model up to the student."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
